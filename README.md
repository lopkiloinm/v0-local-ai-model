<h1 align="center">OpenPrism - Client-Side AI Document Editor</h1>

<p align="center">
  <strong>Fully client-side AI-powered document editor. No servers. No Python. Just Next.js.</strong>
</p>

<p align="center">
  <a href="#-features">Features</a> Â·
  <a href="#-getting-started">Getting Started</a> Â·
  <a href="#-how-it-works">How It Works</a> Â·
  <a href="#-comparison">Comparison</a> Â·
  <a href="#-deployment">Deployment</a>
</p>

<p align="center">
  <a href="https://github.com/yourusername/openprism/stargazers"><img src="https://img.shields.io/github/stars/yourusername/openprism?color=ffcb47&labelColor=black&style=flat-square" alt="Stars"></a>
  <a href="https://github.com/yourusername/openprism/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue?labelColor=black&style=flat-square" alt="License"></a>
  <a href="https://nextjs.org"><img src="https://img.shields.io/badge/Next.js-16-black?style=flat-square&logo=next.js&logoColor=white" alt="Next.js"></a>
  <a href="https://webgpu.io"><img src="https://img.shields.io/badge/WebGPU-Enabled-purple?style=flat-square&logo=webgpu&logoColor=white" alt="WebGPU"></a>
</p>

---

## ğŸ¯ What is OpenPrism?

OpenPrism is a **fully client-side AI document editor** that runs entirely in your browser. Unlike traditional AI applications that require servers, Python environments, or local model installations, OpenPrism brings everything to the client using WebGPU and modern browser APIs.

### Key Philosophy

- âœ… **100% Client-Side**: Everything runs in your browser - no backend servers
- âœ… **Zero Setup**: No Python, no ONNX servers, no ollama.cpp installation
- âœ… **Privacy-First**: Your documents never leave your device
- âœ… **Resource Efficient**: Powered by LFM 2.5 1.2B model - small size, fast inference
- âœ… **User-Friendly**: Just open the app and start writing

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¤– **AI Chat Assistant** | Client-side AI powered by LFM 2.5 1.2B model via WebGPU |
| ğŸ“ **LaTeX Editor** | Real-time LaTeX editing with live preview |
| ğŸ“„ **PDF Preview** | Integrated PDF viewer for document preview |
| ğŸ¨ **Modern UI** | Beautiful, responsive interface built with Radix UI |
| ğŸŒ“ **Dark Mode** | Built-in theme switching |
| ğŸ’¾ **Local Storage** | All documents stored locally in your browser |
| âš¡ **Fast Inference** | Optimized WebGPU inference for quick responses |
| ğŸ”’ **Privacy** | Zero data transmission - everything stays local |

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 18+ (or use pnpm/yarn)
- **Modern Browser** with WebGPU support:
  - Chrome/Edge 113+
  - Firefox 110+ (experimental)
  - Safari 18+ (experimental)

### Installation

Choose your preferred package manager:

**Using npm:**
```bash
npm install
```

**Using pnpm:**
```bash
pnpm install
```

**Using yarn:**
```bash
yarn install
```

### Development

Start the development server:

**With npm:**
```bash
npm run dev
```

**With pnpm:**
```bash
pnpm dev
```

**With yarn:**
```bash
yarn dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

### First Run

1. The app will automatically detect WebGPU support
2. On first use, the AI model (~1.2GB) will download to your browser cache
3. Subsequent visits use the cached model - no re-download needed
4. Start writing and chatting with the AI assistant!

---

## ğŸ”§ How It Works

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Browser (Client)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Next.js App (React)                    â”‚
â”‚  â”œâ”€â”€ LaTeX Editor                       â”‚
â”‚  â”œâ”€â”€ PDF Preview                        â”‚
â”‚  â””â”€â”€ AI Chat Interface                  â”‚
â”‚                                         â”‚
â”‚  WebGPU Runtime                         â”‚
â”‚  â”œâ”€â”€ LFM 2.5 1.2B Model (ONNX)         â”‚
â”‚  â””â”€â”€ Hugging Face Transformers.js       â”‚
â”‚                                         â”‚
â”‚  Browser Cache API                      â”‚
â”‚  â””â”€â”€ Model files cached locally         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **Framework**: Next.js 16 with React 19
- **AI Runtime**: Hugging Face Transformers.js
- **Model**: LFM 2.5 1.2B-Instruct-ONNX (Q4 quantized)
- **Acceleration**: WebGPU for GPU-accelerated inference
- **Document Rendering**: Typst & LaTeX.js
- **UI Components**: Radix UI + Tailwind CSS

### Model Details

- **Model**: LiquidAI/LFM2.5-1.2B-Instruct-ONNX
- **Size**: ~1.2GB (Q4 quantized)
- **Format**: ONNX for browser compatibility
- **Inference**: WebGPU-accelerated
- **Caching**: Browser Cache API + IndexedDB

---

## ğŸ“Š Comparison

### vs. Prismer.AI

| Feature | OpenPrism | Prismer.AI |
|---------|:---------:|:----------:|
| **Setup Required** | âŒ None - just open browser | âœ… ollama.cpp, Python, local installation |
| **Server Required** | âŒ No servers needed | âœ… Backend servers for some features |
| **Python Required** | âŒ Pure JavaScript/TypeScript | âœ… Python for model execution |
| **Local Installation** | âŒ Runs in browser | âœ… Requires local model setup |
| **Privacy** | âœ… 100% local - zero data leaves device | âš ï¸ May require cloud services |
| **Resource Usage** | âœ… Optimized 1.2B model | âš ï¸ Larger models, more resources |
| **Deployment** | âœ… Static hosting (Vercel, Netlify) | âš ï¸ Requires server infrastructure |
| **User Experience** | âœ… Zero configuration | âš ï¸ Setup and configuration needed |

### vs. Traditional AI Apps

| Feature | OpenPrism | Traditional AI Apps |
|---------|:---------:|:------------------:|
| **Backend Servers** | âŒ None | âœ… Required |
| **API Keys** | âŒ Not needed | âœ… Required |
| **Data Privacy** | âœ… Complete privacy | âš ï¸ Data sent to servers |
| **Offline Support** | âœ… Works offline (after first load) | âŒ Requires internet |
| **Cost** | âœ… Free - no API costs | âš ï¸ Pay-per-use or subscription |

---

## ğŸš€ Deployment

### Vercel (Recommended)

1. Push your code to GitHub
2. Import your repository on [Vercel](https://vercel.com)
3. Vercel will auto-detect Next.js
4. Deploy with one click!

Or use the Vercel CLI:
```bash
npm i -g vercel
vercel
```

### Other Platforms

**Netlify:**
```bash
npm run build
# Deploy the .next folder
```

**Static Export:**
```bash
# Add to next.config.mjs:
output: 'export'

npm run build
# Deploy the 'out' folder
```

**Docker:**
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

---

## ğŸ› ï¸ Development

### Project Structure

```
â”œâ”€â”€ app/                    # Next.js app directory
â”‚   â”œâ”€â”€ page.tsx           # Main page
â”‚   â””â”€â”€ layout.tsx         # Root layout
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ ide/              # IDE components
â”‚   â”‚   â”œâ”€â”€ ai-chat.tsx   # AI chat interface
â”‚   â”‚   â”œâ”€â”€ latex-editor.tsx
â”‚   â”‚   â””â”€â”€ pdf-preview.tsx
â”‚   â””â”€â”€ ui/               # UI components
â”œâ”€â”€ lib/                   # Utilities
â”‚   â””â”€â”€ webgpu-model.ts   # AI model loading & inference
â””â”€â”€ public/               # Static assets
```

### Available Scripts

```bash
npm run dev      # Start development server
npm run build    # Build for production
npm run start    # Start production server
npm run lint     # Run ESLint
```

### WebGPU Support

The app requires WebGPU for AI inference. Check support:

```javascript
if (navigator.gpu) {
  console.log('WebGPU supported!');
} else {
  console.log('WebGPU not available');
}
```

---

## ğŸ”’ Privacy & Security

- **Zero Data Transmission**: All AI processing happens locally
- **No Tracking**: No analytics or user tracking
- **Local Storage**: Documents stored in browser only
- **No API Keys**: No external services required
- **Open Source**: Fully auditable codebase

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## â­ Star Us

If you find OpenPrism helpful, please consider giving us a star! It helps us grow and improve.

---

## ğŸ™ Acknowledgments

- [Hugging Face Transformers.js](https://github.com/huggingface/transformers.js) for browser-based AI
- [LiquidAI](https://huggingface.co/LiquidAI) for the LFM 2.5 model
- [Next.js](https://nextjs.org) for the amazing framework
- [Radix UI](https://www.radix-ui.com) for accessible components

---

<p align="center">
  <sub>Built for privacy, simplicity, and performance.</sub>
</p>
